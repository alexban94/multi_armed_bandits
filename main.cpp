#include <iostream>
#include <random>
#include <algorithm> // max_element
#include <cmath> // abs
#include <vector>
#include <iomanip>

using namespace std;

// To be returned by the BanditProblem run function.
struct BanditResults {
        double epsilon;
        vector<double> optimal_accuracy;
        vector<double> reward_log;
    };

// Represents a singular bandit problem generated by the seed.
class BanditProblem {
    private:
        int iterations, t, k, best_a;
        double epsilon;
        vector<double> q, q_a;
        vector<int> n;

        default_random_engine generator;
        vector<normal_distribution<double>> reward_distributions;

    public:
    
    // Constructor.
    BanditProblem(int k, double epsilon, int iterations, unsigned int seed = 0){
        this->k = k;
        this->iterations = iterations;
        this->epsilon = epsilon;

        // Initialize n and q for k bandits/actions.
        n.assign(k, 0);
        q.assign(k, 0);  

        /* For this problem, each action a from 1 to k has an expected reward q*(a) =  E[R_t | A_t = a].
        R_t is the reward at timestep t, and A_t is the action taken at that timestep. To emulate a problem,
        q*(a) can be sampled from the Normal distrubtion. */
        generator.seed(seed);
        normal_distribution<double> distribution(0, 1);
        q_a.assign(k, 0);
        for(int i=0; i<k; i++){
            q_a[i] = distribution(generator);
            reward_distributions.push_back(normal_distribution<double>{q_a[i], 1});
        }

        // Save argmax q_a as the best action - the one that should be chosen for the best reward.
        this->best_a = distance(q_a.begin(), max_element(q_a.begin(), q_a.end()));       

    }
    
    // Run the problem for specifed number of iterations in the constructor.
    BanditResults run(){

        int a = 0; // the action chosen at timestep t.
        double r = 0; // reward for action a.
        // Statistics for evaluation.
        vector<double> reward_log(iterations+1, 0); // For calculating average reward at timestep t.
        vector<double>  optimal_accuracy(iterations+1, 0); // For calculating the accuracy of choosing the best action at t.

        bool best_a_taken[iterations+1] = {false}; // True if the optimal action was taken at timestep t.

        // Uniform distribution for chance of exploration.
        uniform_real_distribution<double> uniform_explore(0,1);
        // Uniform distribution for selecting a random action.
        uniform_int_distribution<int> uniform_int(0, k-1); // a <= sample <= b, so need to sample up to k-1.

        for(int t=0; t<=iterations; t++){
            // If probability < 1 - epsilon, use argmax Q(a) to select best action.
            if(uniform_explore(generator) < 1.0 - epsilon && t > 0){
                a = distance(q.begin(), max_element(q.begin(), q.end()));

            } else {
                // Select a random action.
                a = uniform_int(generator);
            }

            // Obtain reward for taking chosen action a.
            r = bandit(a);

            // If best_a is equal to a, then the best action was selected.
            if(best_a == a){
                best_a_taken[t] = true;
            }
    
            // Update optimal_action using optimal a. This stores the current % accuracy the best action was taken at each t.
            if(t > 0){
                optimal_accuracy[t] = ((double) accumulate(best_a_taken, best_a_taken + t, 0)) * 100/t; //  Sum from first element up to current element at time t. Divide by t for accuracy.
            } else {
                // Avoid division by 0 on first timestep. 
                optimal_accuracy[t] = ((double) accumulate(best_a_taken, best_a_taken + t, 0)) * 100;
                
            }
            
            // Add r to the reward log for this timestep t.
            reward_log[t] = r;
            
            // Only the reward r of the chosen action a is used in updating the corresponding N(a) and Q(a) on this timestep.
            // Update "step-size" parameter (1/n) for sample-average method of estimating action-values.
            n[a] += 1;
            q[a] = q[a] + ((1.0/n[a]) * (r - q[a]));
        }
        
        // Return results.
        BanditResults result;
        result.optimal_accuracy = optimal_accuracy;
        result.reward_log = reward_log;
        result.epsilon = epsilon;
        return result;
        
    }

    // Sample from a normal distribution with mean = q*(a) for the chosen action a, to obtain reward for current timestep.
    double bandit(int action){
        return reward_distributions[action](generator);
    }

};

// Print stats for a singular run (deprecated).
void print_stats(vector<double> &avg_reward, vector<double> &avg_optimal_accuracy){
    cout<<"Average reward over 1000 timesteps:"<<endl;
    for(int i=0; i < (int) avg_reward.size(); i++)
        cout <<i<<": "<<avg_reward.at(i) <<endl;

    cout<<"Average optimal action accuracy over 1000 timesteps:"<<endl;
    for(int i=0; i < (int) avg_optimal_accuracy.size(); i++)
        cout <<i<<": "<<avg_optimal_accuracy.at(i) <<"\%"<<endl;
}

// Print line for output formatting.
void print_line(){
    for(int i=0; i<40; i++){
        cout<<"--";
    }
    cout<<endl;
}

// Prints progress of averaged results every i iterations.
void print_tabular(vector<BanditResults> result_vector, int k, int interval=100, int timesteps=1000, int n_experiments=2000){
    // Average reward printout.
    print_line();
    cout<<"Average reward over "<<n_experiments<<" runs using k = "<<k<<" and "<<timesteps<<" timesteps:"<<endl;
    print_line();
    cout<<"timestep";
    for(int i=0; i< (int) result_vector.size(); i++){
        // Print columns using epsilon values.
        cout<<"\t\t"<<result_vector[i].epsilon;
    }
    cout<<endl;
    print_line();
    // Print results of each test for current timestep.
    for(int i=0; i <= timesteps; i+=interval){
        cout<<"  "<<i<<"\t";
        for(int j=0; j< (int) result_vector.size(); j++){
            cout<<"\t\t"<<result_vector[j].reward_log[i];
        }
        cout<<endl;
    }

    // Average optimal accuracy printout.
    cout<<endl;
    print_line();
    cout<<"Average optimal action accuracy over "<<n_experiments<<" runs using k = "<<k<<" and "<<timesteps<<" timesteps:"<<endl;
    print_line();
    cout<<"timestep";
    for(int i=0; i< (int) result_vector.size(); i++){
        // Print columns using epsilon values.
        cout<<"\t\t"<<result_vector[i].epsilon;
    }
    cout<<endl;
    print_line();
    // Print results of each test for current timestep.
    for(int i=0; i <= timesteps; i+=interval){
        cout<<"  "<<i<<"\t";
        for(int j=0; j< (int) result_vector.size(); j++){
            cout<<"\t\t"<<result_vector[j].optimal_accuracy[i]<<"\%";
        }
        cout<<endl;
    }
}

// Runs all n_experiments and returns the averaged results.
BanditResults run_experiment_config(int k, double epsilon, int timesteps, int n_experiments){
    
    // Vectors to tally average results.
    vector<double> avg_reward(timesteps+1,0), avg_optimal_accuracy(timesteps+1,0);
    BanditResults result;

    // Run bandit trials.
    for(int n=0; n<n_experiments; n++){
        //cout<<"Trial "<<n<<endl;
        result = BanditProblem(k, epsilon, timesteps+1, n).run();
        // Add current results to the total.
        for(int i=0; i<=timesteps; i++){
            avg_optimal_accuracy[i] += result.optimal_accuracy[i];
            avg_reward[i] += result.reward_log[i];
        }
    }

    // Average results by dividing by n_experiments.
    // First two parameters give the range of elements to transform.
    // Third parameter is the intial iterator to where to store values after transform.
    // Fourth is the lambda function: takes n_experiments as a "captured" variable from the enclosing scope;
    // takes a double  reference as a parameter (each vector element). Followed by the function, returning the division.
    transform(avg_optimal_accuracy.begin(), avg_optimal_accuracy.end(), avg_optimal_accuracy.begin(), [n_experiments](double &c){ return c/n_experiments; });
    transform(avg_reward.begin(), avg_reward.end(), avg_reward.begin(), [n_experiments](double &c){ return c/n_experiments; });
        
    BanditResults avg_result;
    avg_result.optimal_accuracy = avg_optimal_accuracy;
    avg_result.reward_log = avg_reward;
    avg_result.epsilon = epsilon;
    return avg_result;
  
}

int main(){

    // Set print precision.
    cout<<setprecision(4);
    // Number of experiments to perform.
    const int n_experiments = 2000;
    const int timesteps = 1000;
    const int k = 10;
    // Values of epsilon to use.
    const double e[3] = {0, 0.01, 0.1};

    // Peform trial experiments.
    BanditResults e1 = run_experiment_config(k, e[0], timesteps, n_experiments);
    BanditResults e2 = run_experiment_config(k, e[1], timesteps, n_experiments);
    BanditResults e3 = run_experiment_config(k, e[2], timesteps, n_experiments);
        
    vector<BanditResults> result_vector = {e1, e2, e3};    
    print_tabular(result_vector, k, 100, timesteps, n_experiments);

    return 0; 
}
